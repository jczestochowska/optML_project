{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l9bhe4O20wC4"
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xbTxvNyb3y2O"
   },
   "outputs": [],
   "source": [
    "if not os.path.isdir(\"./data\"):\n",
    "    os.mkdir(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Kj2BrCW1Ljs"
   },
   "source": [
    "###DATA UTILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w0ZImG-a03c-"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, Subset\n",
    "from torchvision import datasets\n",
    "\n",
    "\n",
    "def get_data_loaders(batch_size, num_clients, iid_split=True, percentage_val=0.2, full=False,\n",
    "                     non_iid_mix=0):\n",
    "    val_loader = None\n",
    "    train_input, train_target, test_input, test_target = load_data(flatten=False, full=full)\n",
    "    train_dataset = TensorDataset(train_input, train_target)\n",
    "\n",
    "    # If validation set is needed randomly split training set\n",
    "    if percentage_val:\n",
    "        val_dataset, train_dataset = torch.utils.data.random_split(train_dataset,\n",
    "                                                                   (int(percentage_val * len(train_dataset)),\n",
    "                                                                    int((1 - percentage_val) * len(train_dataset)))\n",
    "                                                                   )\n",
    "        val_loader = DataLoader(dataset=val_dataset,\n",
    "                                batch_size=batch_size,\n",
    "                                shuffle=True)\n",
    "    # Split data for each client\n",
    "    if iid_split:\n",
    "        # Random IID data split\n",
    "        client_datasets = torch.utils.data.random_split(train_dataset, np.tile(int(len(train_dataset) / num_clients),\n",
    "                                                                               num_clients).tolist())\n",
    "    else:\n",
    "        if non_iid_mix:\n",
    "            non_iid_part, iid_part = get_non_iid_split(train_dataset, non_iid_mix)\n",
    "            client_datasets = get_non_iid_datasets(num_clients, non_iid_part)  # make client_datasets with non_iid_part\n",
    "            for client_nr, client_dataset in enumerate(client_datasets):\n",
    "                chunk_size = int(len(iid_part) / num_clients)\n",
    "                client_dataset.indices.\\\n",
    "                    extend(iid_part.indices[chunk_size*client_nr: chunk_size*(1+client_nr)])\n",
    "        else:\n",
    "            # Each client has different set of non overlapping digits\n",
    "            client_datasets = get_non_iid_datasets(num_clients, train_dataset)\n",
    "    random.shuffle(client_datasets)\n",
    "    train_loaders = []\n",
    "    for train_dataset in client_datasets:\n",
    "        train_loader = DataLoader(dataset=train_dataset,\n",
    "                                  batch_size=batch_size,\n",
    "                                  shuffle=True)\n",
    "        train_loaders.append(train_loader)\n",
    "\n",
    "    test_loader = DataLoader(dataset=TensorDataset(test_input, test_target),\n",
    "                             batch_size=batch_size)\n",
    "\n",
    "    return train_loaders, val_loader, test_loader\n",
    "\n",
    "\n",
    "def get_non_iid_split(train_dataset, non_iid_mix_p):\n",
    "    # split train_dataset into a non-iid and iid part\n",
    "    iid_part, non_iid_part = torch.utils.data.random_split(train_dataset, [round(non_iid_mix_p * len(train_dataset)),\n",
    "                                                                           round((1 - non_iid_mix_p) * len(\n",
    "                                                                               train_dataset))])\n",
    "    if isinstance(train_dataset, Subset):\n",
    "        iid_part.dataset = iid_part.dataset.dataset\n",
    "        non_iid_part.dataset = non_iid_part.dataset.dataset\n",
    "    return non_iid_part, iid_part\n",
    "\n",
    "\n",
    "def load_data(cifar=False, one_hot_labels=False, normalize=False, flatten=False, full=False):\n",
    "    data_dir = './data'\n",
    "\n",
    "    if cifar:\n",
    "        print('* Using CIFAR')\n",
    "        cifar_train_set = datasets.CIFAR10(data_dir + '/cifar10/', train=True, download=True)\n",
    "        cifar_test_set = datasets.CIFAR10(data_dir + '/cifar10/', train=False, download=True)\n",
    "\n",
    "        train_input = torch.from_numpy(cifar_train_set.data)\n",
    "        train_input = train_input.transpose(3, 1).transpose(2, 3).float()\n",
    "        train_target = torch.tensor(cifar_train_set.targets, dtype=torch.int64)\n",
    "\n",
    "        test_input = torch.from_numpy(cifar_test_set.data).float()\n",
    "        test_input = test_input.transpose(3, 1).transpose(2, 3).float()\n",
    "        test_target = torch.tensor(cifar_test_set.targets, dtype=torch.int64)\n",
    "\n",
    "    else:\n",
    "        print('* Using MNIST')\n",
    "        mnist_train_set = datasets.MNIST(data_dir + '/mnist/', train=True, download=True)\n",
    "        mnist_test_set = datasets.MNIST(data_dir + '/mnist/', train=False, download=True)\n",
    "\n",
    "        train_input = mnist_train_set.data.view(-1, 1, 28, 28).float()\n",
    "        train_target = mnist_train_set.targets\n",
    "        test_input = mnist_test_set.data.view(-1, 1, 28, 28).float()\n",
    "        test_target = mnist_test_set.targets\n",
    "\n",
    "    if flatten:\n",
    "        train_input = train_input.clone().reshape(train_input.size(0), -1)\n",
    "        test_input = test_input.clone().reshape(test_input.size(0), -1)\n",
    "\n",
    "    if not full:\n",
    "        print('** Reducing the data-set, (use --full for the full thing)')\n",
    "        train_input = train_input.narrow(0, 0, 5000)\n",
    "        train_target = train_target.narrow(0, 0, 5000)\n",
    "        test_input = test_input.narrow(0, 0, 5000)\n",
    "        test_target = test_target.narrow(0, 0, 5000)\n",
    "\n",
    "    print('** Use {:d} train and {:d} test samples'.format(train_input.size(0), test_input.size(0)))\n",
    "\n",
    "    if one_hot_labels:\n",
    "        train_target = convert_to_one_hot_labels(train_input, train_target)\n",
    "        test_target = convert_to_one_hot_labels(test_input, test_target)\n",
    "\n",
    "    if normalize:\n",
    "        mu, std = train_input.mean(), train_input.std()\n",
    "        train_input.sub_(mu).div_(std)\n",
    "        test_input.sub_(mu).div_(std)\n",
    "\n",
    "    return train_input, train_target, test_input, test_target\n",
    "\n",
    "\n",
    "def convert_to_one_hot_labels(input, target):\n",
    "    tmp = input.new_zeros(target.size(0), target.max() + 1)\n",
    "    tmp.scatter_(1, target.view(-1, 1), 1.0)\n",
    "    return tmp\n",
    "\n",
    "\n",
    "def get_non_iid_datasets(num_clients, train_dataset):\n",
    "    \"\"\"\n",
    "    This function divides samples in a way that\n",
    "    each client has non-overlapping classes,\n",
    "    e.g client 1 has only digits 0 and 1 while client 2 has only digits 2 and 3.\n",
    "    To achieve this we perform binary search on labels tensor\n",
    "    to divide initial dataset\n",
    "    \"\"\"\n",
    "    client_datasets = []\n",
    "    # if we have validation set then train is a Subset type\n",
    "    if isinstance(train_dataset, Subset):\n",
    "        labels = train_dataset.dataset.tensors[1][train_dataset.indices]\n",
    "    else:\n",
    "        labels = train_dataset.tensors[1]\n",
    "    labels, sorted_indices = torch.sort(labels)\n",
    "    digits_per_client = 10 // num_clients\n",
    "    digit = 0\n",
    "    for client in range(num_clients):\n",
    "        first_idx = first_index(labels, 0, len(labels), digit)\n",
    "        if client == num_clients - 1:\n",
    "            last_idx = len(labels) - 1\n",
    "        else:\n",
    "            last_idx = last_index(labels, 0, len(labels), digit + (digits_per_client - 1))\n",
    "        if isinstance(train_dataset, Subset):\n",
    "            new_indices = np.array(train_dataset.indices)[sorted_indices[first_idx: last_idx + 1].numpy()]\n",
    "            client_dataset = Subset(train_dataset.dataset, new_indices.tolist())\n",
    "        else:\n",
    "            client_dataset = Subset(train_dataset, sorted_indices[first_idx: last_idx + 1].tolist())\n",
    "        client_datasets.append(client_dataset)\n",
    "        digit += digits_per_client\n",
    "    return client_datasets\n",
    "\n",
    "\n",
    "# binary search functions to retrieve first and last index of label in sorted labels array\n",
    "def first_index(array, low, high, item):\n",
    "    if high >= low:\n",
    "        mid = low + (high - low) // 2\n",
    "        if (mid == 0 or item > array[mid - 1]) and array[mid] == item:\n",
    "            return mid\n",
    "        elif item > array[mid]:\n",
    "            return first_index(array, (mid + 1), high, item)\n",
    "        else:\n",
    "            return first_index(array, low, (mid - 1), item)\n",
    "    print(f\"This label {item} was not found\")\n",
    "    return -1\n",
    "\n",
    "\n",
    "def last_index(array, low, high, item):\n",
    "    if high >= low:\n",
    "        mid = low + (high - low) // 2\n",
    "        if (mid == len(array) - 1 or item < array[mid + 1]) and array[mid] == item:\n",
    "            return mid\n",
    "        elif item < array[mid]:\n",
    "            return last_index(array, low, (mid - 1), item)\n",
    "        else:\n",
    "            return last_index(array, (mid + 1), high, item)\n",
    "    print(f\"This label {item} was not found\")\n",
    "    return -1\n",
    "\n",
    "\n",
    "def get_model_bits(state_dict):\n",
    "    \"\"\"\n",
    "    :param state_dict: model object for which we want to get size in bits\n",
    "    :return: model_size - number of bites for all model's parameters\n",
    "    \"\"\"\n",
    "    torch.save(state_dict, \"temp.p\")\n",
    "    # Multiply by 8 to go from bytes to bits\n",
    "    model_size = os.path.getsize(\"temp.p\") * 8\n",
    "    os.remove('temp.p')\n",
    "    return model_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ePo62fMS1TSp"
   },
   "source": [
    "###QUANTIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wdNFSP_G1SnU"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def quantize_float16(model_dict):\n",
    "    \"\"\"\n",
    "    :param model: Model's state dict with default 32-bit float parameters\n",
    "    :return: model's state dict with 16-bit float parameters\n",
    "    \"\"\"\n",
    "    for name, param in model_dict.items():\n",
    "        model_dict[name] = param.half()\n",
    "    return model_dict\n",
    "\n",
    "\n",
    "def quantize_int8(model_dict):\n",
    "    # Find maximum parameter\n",
    "    max_param = 0\n",
    "    for name, param in model_dict.items():\n",
    "        new_max = param.abs().max()\n",
    "        if new_max > max_param:\n",
    "            max_param = new_max\n",
    "    # Scale the maximum value to the max of an int8\n",
    "    multiplier = 127 / max_param\n",
    "    for name, param in model_dict.items():\n",
    "        model_dict[name] = (param * multiplier).to(torch.int8)\n",
    "    return model_dict, multiplier\n",
    "\n",
    "\n",
    "def decode_quantized_model_int8(model_dict, multiplier):\n",
    "    for name, param in model_dict.items():\n",
    "        model_dict[name] = param.to(torch.float32) / multiplier\n",
    "    return model_dict\n",
    "\n",
    "\n",
    "def no_quantization(model_dict):\n",
    "    return model_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mjOT9RKT1lCt"
   },
   "source": [
    "###MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9qilPkNt1kqX"
   },
   "outputs": [],
   "source": [
    "from torch import nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "# https://arxiv.org/pdf/1602.05629.pdf\n",
    "# A CNN with two 5x5 convolution layers (the first with\n",
    "# 32 channels, the second with 64, each followed with 2x2\n",
    "# max pooling), a fully connected layer with 512 units and\n",
    "# ReLu activation, and a final softmax output layer (1,663,370\n",
    "# total parameters).\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 5)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 5)\n",
    "        self.fc1 = nn.Linear(4 * 4 * 64, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4 * 4 * 64)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1tONngmM1eyD"
   },
   "source": [
    "###TRAINING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2KCwK2211eIN"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class Client:\n",
    "    def __init__(self, data_loader, epochs=5):\n",
    "        self.data_loader = data_loader\n",
    "        self.epochs = epochs\n",
    "        self.lr = 0.001\n",
    "        self.log_interval = 5\n",
    "        self.seed = 42\n",
    "        torch.manual_seed(self.seed)\n",
    "        self.save_model = False\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = CNN().to(self.device)\n",
    "        self.optimizer = optim.SGD(self.model.parameters(), lr=self.lr)\n",
    "        self.gradient_compression = None\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        self.model_name = \"mnist_cnn\"\n",
    "\n",
    "\n",
    "def train(client, epoch, logging=True):\n",
    "    # put model in train mode, we need gradients\n",
    "    client.model.train()\n",
    "    train_loader = client.data_loader\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        client.optimizer.zero_grad()\n",
    "        output = client.model(data)\n",
    "        # get the basic loss for our main task\n",
    "        total_loss = client.criterion(output, target)\n",
    "        total_loss.backward()\n",
    "        train_loss += total_loss.item()\n",
    "        client.optimizer.step()\n",
    "    _, train_accuracy = test(client, logging=False)\n",
    "    if logging:\n",
    "        print(f'Train Epoch: {epoch} Loss: {total_loss.item():.6f}, Train accuracy: {train_accuracy}')\n",
    "    return train_loss, train_accuracy\n",
    "\n",
    "\n",
    "def test(client, logging=True):\n",
    "    # put model in eval mode, disable dropout etc.\n",
    "    client.model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    test_loader = client.data_loader\n",
    "    # disable grad to perform testing quicker\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            data, target = data.to(client.device), target.to(client.device)\n",
    "            output = client.model(data)\n",
    "            test_loss += client.criterion(output, target).item()\n",
    "            # prediction is an output with maximal probability\n",
    "            pred = output.argmax(1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    if logging:\n",
    "        print(f'Test set: Average loss: {test_loss:.4f}, '\n",
    "              f'Test accuracy: {correct} / {len(test_loader.dataset)} '\n",
    "              f'({test_accuracy:.0f}%)\\n')\n",
    "    return test_loss, test_accuracy\n",
    "\n",
    "\n",
    "def average_client_models(clients_dicts):\n",
    "    \"\"\"\n",
    "    :param clients_dicts: list of clients state dicts\n",
    "    :return: state_dict of averaged parameters\n",
    "    \"\"\"\n",
    "    # To perform averaging we need to go back to float32 cause summing is not supported for float16\n",
    "    for client in clients_dicts:\n",
    "        for name, param in client.items():\n",
    "            client[name] = param.float()\n",
    "    dict_keys = clients_dicts[0].keys()\n",
    "    final_dict = dict.fromkeys(dict_keys)\n",
    "    for key in dict_keys:\n",
    "        # Average model parameters\n",
    "        final_dict[key] = torch.cat([dictionary[key].unsqueeze(0) for dictionary in clients_dicts], dim=0).sum(0).div(\n",
    "            len(clients_dicts))\n",
    "    return final_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6-wtjQX-1sWP"
   },
   "source": [
    "### EXPERIMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kUJUI7iy16aG"
   },
   "outputs": [],
   "source": [
    "# Create outputs directory when running in colab for the first time\n",
    "if not os.path.isdir(\"./outputs\"):\n",
    "    os.mkdir(\"./outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000.0
    },
    "colab_type": "code",
    "id": "yLhdO_zY1gy1",
    "outputId": "5f7314e7-3c6b-4147-bf26-e3729306f81f"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "##############################\n",
    "# Configure here to get a specific experiment\n",
    "batch_size = 25\n",
    "num_clients = 5\n",
    "target_accuracy = 93\n",
    "iid_split = True\n",
    "non_iid_mix = 0\n",
    "# validation set\n",
    "percentage_val = 0\n",
    "# if use full MNIST with 60000 train and 10000 test\n",
    "full = True\n",
    "# default setup is 5 epochs per client,\n",
    "# here we have five clients therefore  we need [5, 5, 5, 5, 5]\n",
    "# change the list accordingly to get variable\n",
    "# number of epochs for different clients\n",
    "client_epochs = 5\n",
    "epochs_per_client = num_clients * [client_epochs]\n",
    "quantization = quantize_int8\n",
    "##############################\n",
    "\n",
    "\n",
    "# Load data\n",
    "train_loaders, _, test_loader = get_data_loaders(batch_size, num_clients, non_iid_mix=non_iid_mix,\n",
    "                                                 percentage_val=percentage_val, iid_split=iid_split, full=full)\n",
    "\n",
    "# Initialize all clients\n",
    "clients = [Client(train_loader, epochs) for train_loader, epochs in zip(train_loaders, epochs_per_client)]\n",
    "\n",
    "# Set seed for the script\n",
    "torch.manual_seed(clients[0].seed)\n",
    "\n",
    "testing_accuracy = 0\n",
    "num_rounds = 1\n",
    "\n",
    "central_server = Client(test_loader)\n",
    "\n",
    "filename = f\"iid_split_{iid_split}_quantization_{quantization.__name__}_num_epochs_{client_epochs}_mix_{non_iid_mix}.pkl\"\n",
    "experiment_state = {\"num_rounds\": 0,\n",
    "                    \"test_accuracies\": [],\n",
    "                    \"conserved_bits_from_server\": [],\n",
    "                    \"conserved_bits_from_clients\": [],\n",
    "                    \"transferred_bits_from_server\": [],\n",
    "                    \"transferred_bits_from_clients\": [],\n",
    "                    \"original_bits_from_server\": [],\n",
    "                    \"original_bits_from_clients\": []\n",
    "                    }\n",
    "\n",
    "# Multiplier for int8 quantization\n",
    "multiplier = 0\n",
    "\n",
    "while testing_accuracy < target_accuracy:\n",
    "    print(\"Communication Round {0}\".format(num_rounds))\n",
    "\n",
    "    if num_rounds > 1:\n",
    "        # Load server weights onto clients\n",
    "        total_bits_conserved = 0\n",
    "        total_bits_transferred = 0\n",
    "        total_float_model_bits = 0\n",
    "        for client in clients:\n",
    "            with torch.no_grad():\n",
    "                # Calculate number of bits in full server model\n",
    "                float_model_bits = get_model_bits(central_server.model.state_dict())\n",
    "                # Quantize server's model\n",
    "                if quantization == quantize_int8:\n",
    "                    quantized_model, multiplier = quantization(central_server.model.state_dict())\n",
    "                else:\n",
    "                    quantized_model = quantization(central_server.model.state_dict())\n",
    "                bits_transferred = get_model_bits(quantized_model)\n",
    "                # Calculate how many bits we saved\n",
    "                bits_conserved = (float_model_bits - bits_transferred)\n",
    "                # If quantization method is int8, decode the weights\n",
    "                if quantization == quantize_int8:\n",
    "                    quantized_model = decode_quantized_model_int8(quantized_model, multiplier)\n",
    "                # Distribute quantized model on clients\n",
    "                client.model.load_state_dict(quantized_model)\n",
    "                # Update summary values\n",
    "                total_bits_conserved += bits_conserved\n",
    "                total_bits_transferred += bits_transferred\n",
    "                total_float_model_bits += float_model_bits\n",
    "\n",
    "        # Add to our summary\n",
    "        experiment_state[\"conserved_bits_from_server\"].append(total_bits_conserved // num_clients)\n",
    "        experiment_state[\"transferred_bits_from_server\"].append(total_bits_transferred // num_clients)\n",
    "        experiment_state[\"original_bits_from_server\"].append(total_float_model_bits // num_clients)\n",
    "\n",
    "    # Perform E local training steps for each client\n",
    "    for client_idx, client in enumerate(clients):\n",
    "        print(\"Training client {0}\".format(client_idx))\n",
    "        for epoch in range(1, client.epochs + 1):\n",
    "            train(client, epoch)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Get number of bits in all clients' models before quantization\n",
    "        clients_bits = reduce((lambda x, y: x + y), [get_model_bits(client.model.state_dict()) for client in clients])\n",
    "        # Quantize clients models\n",
    "        if quantization == quantize_int8:\n",
    "            quantized_clients_models = []\n",
    "            multipliers = []\n",
    "            for client in clients:\n",
    "                client_model, multiplier = quantization(client.model.state_dict())\n",
    "                quantized_clients_models.append(client_model)\n",
    "                multipliers.append(multiplier)\n",
    "        else:\n",
    "            quantized_clients_models = [quantization(client.model.state_dict()) for client in clients]\n",
    "        quantized_clients_bits = reduce((lambda x, y: x + y),\n",
    "                                        [get_model_bits(client) for client in quantized_clients_models])\n",
    "        bits_conserved = (clients_bits - quantized_clients_bits) // num_clients\n",
    "        # Add to summary\n",
    "        experiment_state[\"conserved_bits_from_clients\"].append(bits_conserved)\n",
    "        experiment_state[\"transferred_bits_from_clients\"].append(quantized_clients_bits // num_clients)\n",
    "        experiment_state[\"original_bits_from_clients\"].append(clients_bits // num_clients)\n",
    "        # Decode bits on central server side:\n",
    "        if quantization == quantize_int8:\n",
    "            new_client_models = []\n",
    "            for client, multiplier in zip(quantized_clients_models, multipliers):\n",
    "                new_client = decode_quantized_model_int8(client, multiplier)\n",
    "                new_client_models.append(new_client)\n",
    "            quantized_clients_models = new_client_models\n",
    "        # Send quantized models to server and average them\n",
    "        averaged_model = average_client_models(quantized_clients_models)\n",
    "        central_server.model.load_state_dict(averaged_model)\n",
    "    # We have to convert back to float32 otherwise there is a mismatch with input dtype\n",
    "    central_server.model.to(torch.float32)\n",
    "    # Test the aggregated model\n",
    "    test_loss, testing_accuracy = test(central_server)\n",
    "    experiment_state['test_accuracies'].append(testing_accuracy)\n",
    "    experiment_state['num_rounds'] = num_rounds + 1\n",
    "    \n",
    "    # Save experiment states\n",
    "    with open(os.path.join(\"./outputs\", filename), \"wb\") as f:\n",
    "        pickle.dump(experiment_state, f)\n",
    "    \n",
    "    num_rounds += 1\n",
    "    \n",
    "\n",
    "# Save model\n",
    "if central_server.save_model:\n",
    "    torch.save(central_server.model.state_dict(), f\"{central_server.model_name}.pt\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "experiments_colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
