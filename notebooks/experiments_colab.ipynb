{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "experiments_colab.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9bhe4O20wC4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pickle\n",
        "from functools import reduce\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbTxvNyb3y2O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.isdir(\"./data\"):\n",
        "  os.mkdir(\"data\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Kj2BrCW1Ljs",
        "colab_type": "text"
      },
      "source": [
        "###DATA UTILS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0ZImG-a03c-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, Subset\n",
        "from torchvision import datasets\n",
        "\n",
        "\n",
        "def get_data_loaders(batch_size, num_clients, iid_split=True, percentage_val=0.2, full=False,\n",
        "                     non_iid_mix=0):\n",
        "    val_loader = None\n",
        "    train_input, train_target, test_input, test_target = load_data(flatten=False, full=full)\n",
        "    train_dataset = TensorDataset(train_input, train_target)\n",
        "\n",
        "    # If validation set is needed randomly split training set\n",
        "    if percentage_val:\n",
        "        val_dataset, train_dataset = torch.utils.data.random_split(train_dataset,\n",
        "                                                                   (int(percentage_val * len(train_dataset)),\n",
        "                                                                    int((1 - percentage_val) * len(train_dataset)))\n",
        "                                                                   )\n",
        "        val_loader = DataLoader(dataset=val_dataset,\n",
        "                                batch_size=batch_size,\n",
        "                                shuffle=True)\n",
        "    # Split data for each client\n",
        "    if iid_split:\n",
        "        # Random IID data split\n",
        "        client_datasets = torch.utils.data.random_split(train_dataset, np.tile(int(len(train_dataset) / num_clients),\n",
        "                                                                               num_clients).tolist())\n",
        "    else:\n",
        "        if non_iid_mix:\n",
        "            non_iid_part, iid_part = get_non_iid_split(train_dataset, non_iid_mix)\n",
        "            client_datasets = get_non_iid_datasets(num_clients, non_iid_part)  # make client_datasets with non_iid_part\n",
        "            for client_nr, client_dataset in enumerate(client_datasets):\n",
        "                chunk_size = int(len(iid_part) / num_clients)\n",
        "                client_dataset.indices.\\\n",
        "                    extend(iid_part.indices[chunk_size*client_nr: chunk_size*(1+client_nr)])\n",
        "        else:\n",
        "            # Each client has different set of non overlapping digits\n",
        "            client_datasets = get_non_iid_datasets(num_clients, train_dataset)\n",
        "    random.shuffle(client_datasets)\n",
        "    train_loaders = []\n",
        "    for train_dataset in client_datasets:\n",
        "        train_loader = DataLoader(dataset=train_dataset,\n",
        "                                  batch_size=batch_size,\n",
        "                                  shuffle=True)\n",
        "        train_loaders.append(train_loader)\n",
        "\n",
        "    test_loader = DataLoader(dataset=TensorDataset(test_input, test_target),\n",
        "                             batch_size=batch_size)\n",
        "\n",
        "    return train_loaders, val_loader, test_loader\n",
        "\n",
        "\n",
        "def get_non_iid_split(train_dataset, non_iid_mix_p):\n",
        "    # split train_dataset into a non-iid and iid part\n",
        "    iid_part, non_iid_part = torch.utils.data.random_split(train_dataset, [round(non_iid_mix_p * len(train_dataset)),\n",
        "                                                                           round((1 - non_iid_mix_p) * len(\n",
        "                                                                               train_dataset))])\n",
        "    if isinstance(train_dataset, Subset):\n",
        "        iid_part.dataset = iid_part.dataset.dataset\n",
        "        non_iid_part.dataset = non_iid_part.dataset.dataset\n",
        "    return non_iid_part, iid_part\n",
        "\n",
        "\n",
        "def load_data(cifar=False, one_hot_labels=False, normalize=False, flatten=False, full=False):\n",
        "    data_dir = './data'\n",
        "\n",
        "    if cifar:\n",
        "        print('* Using CIFAR')\n",
        "        cifar_train_set = datasets.CIFAR10(data_dir + '/cifar10/', train=True, download=True)\n",
        "        cifar_test_set = datasets.CIFAR10(data_dir + '/cifar10/', train=False, download=True)\n",
        "\n",
        "        train_input = torch.from_numpy(cifar_train_set.data)\n",
        "        train_input = train_input.transpose(3, 1).transpose(2, 3).float()\n",
        "        train_target = torch.tensor(cifar_train_set.targets, dtype=torch.int64)\n",
        "\n",
        "        test_input = torch.from_numpy(cifar_test_set.data).float()\n",
        "        test_input = test_input.transpose(3, 1).transpose(2, 3).float()\n",
        "        test_target = torch.tensor(cifar_test_set.targets, dtype=torch.int64)\n",
        "\n",
        "    else:\n",
        "        print('* Using MNIST')\n",
        "        mnist_train_set = datasets.MNIST(data_dir + '/mnist/', train=True, download=True)\n",
        "        mnist_test_set = datasets.MNIST(data_dir + '/mnist/', train=False, download=True)\n",
        "\n",
        "        train_input = mnist_train_set.data.view(-1, 1, 28, 28).float()\n",
        "        train_target = mnist_train_set.targets\n",
        "        test_input = mnist_test_set.data.view(-1, 1, 28, 28).float()\n",
        "        test_target = mnist_test_set.targets\n",
        "\n",
        "    if flatten:\n",
        "        train_input = train_input.clone().reshape(train_input.size(0), -1)\n",
        "        test_input = test_input.clone().reshape(test_input.size(0), -1)\n",
        "\n",
        "    if not full:\n",
        "        print('** Reducing the data-set, (use --full for the full thing)')\n",
        "        train_input = train_input.narrow(0, 0, 5000)\n",
        "        train_target = train_target.narrow(0, 0, 5000)\n",
        "        test_input = test_input.narrow(0, 0, 5000)\n",
        "        test_target = test_target.narrow(0, 0, 5000)\n",
        "\n",
        "    print('** Use {:d} train and {:d} test samples'.format(train_input.size(0), test_input.size(0)))\n",
        "\n",
        "    if one_hot_labels:\n",
        "        train_target = convert_to_one_hot_labels(train_input, train_target)\n",
        "        test_target = convert_to_one_hot_labels(test_input, test_target)\n",
        "\n",
        "    if normalize:\n",
        "        mu, std = train_input.mean(), train_input.std()\n",
        "        train_input.sub_(mu).div_(std)\n",
        "        test_input.sub_(mu).div_(std)\n",
        "\n",
        "    return train_input, train_target, test_input, test_target\n",
        "\n",
        "\n",
        "def convert_to_one_hot_labels(input, target):\n",
        "    tmp = input.new_zeros(target.size(0), target.max() + 1)\n",
        "    tmp.scatter_(1, target.view(-1, 1), 1.0)\n",
        "    return tmp\n",
        "\n",
        "\n",
        "def get_non_iid_datasets(num_clients, train_dataset):\n",
        "    \"\"\"\n",
        "    This function divides samples in a way that\n",
        "    each client has non-overlapping classes,\n",
        "    e.g client 1 has only digits 0 and 1 while client 2 has only digits 2 and 3.\n",
        "    To achieve this we perform binary search on labels tensor\n",
        "    to divide initial dataset\n",
        "    \"\"\"\n",
        "    client_datasets = []\n",
        "    # if we have validation set then train is a Subset type\n",
        "    if isinstance(train_dataset, Subset):\n",
        "        labels = train_dataset.dataset.tensors[1][train_dataset.indices]\n",
        "    else:\n",
        "        labels = train_dataset.tensors[1]\n",
        "    labels, sorted_indices = torch.sort(labels)\n",
        "    digits_per_client = 10 // num_clients\n",
        "    digit = 0\n",
        "    for client in range(num_clients):\n",
        "        first_idx = first_index(labels, 0, len(labels), digit)\n",
        "        if client == num_clients - 1:\n",
        "            last_idx = len(labels) - 1\n",
        "        else:\n",
        "            last_idx = last_index(labels, 0, len(labels), digit + (digits_per_client - 1))\n",
        "        if isinstance(train_dataset, Subset):\n",
        "            new_indices = np.array(train_dataset.indices)[sorted_indices[first_idx: last_idx + 1].numpy()]\n",
        "            client_dataset = Subset(train_dataset.dataset, new_indices.tolist())\n",
        "        else:\n",
        "            client_dataset = Subset(train_dataset, sorted_indices[first_idx: last_idx + 1].tolist())\n",
        "        client_datasets.append(client_dataset)\n",
        "        digit += digits_per_client\n",
        "    return client_datasets\n",
        "\n",
        "\n",
        "# binary search functions to retrieve first and last index of label in sorted labels array\n",
        "def first_index(array, low, high, item):\n",
        "    if high >= low:\n",
        "        mid = low + (high - low) // 2\n",
        "        if (mid == 0 or item > array[mid - 1]) and array[mid] == item:\n",
        "            return mid\n",
        "        elif item > array[mid]:\n",
        "            return first_index(array, (mid + 1), high, item)\n",
        "        else:\n",
        "            return first_index(array, low, (mid - 1), item)\n",
        "    print(f\"This label {item} was not found\")\n",
        "    return -1\n",
        "\n",
        "\n",
        "def last_index(array, low, high, item):\n",
        "    if high >= low:\n",
        "        mid = low + (high - low) // 2\n",
        "        if (mid == len(array) - 1 or item < array[mid + 1]) and array[mid] == item:\n",
        "            return mid\n",
        "        elif item < array[mid]:\n",
        "            return last_index(array, low, (mid - 1), item)\n",
        "        else:\n",
        "            return last_index(array, (mid + 1), high, item)\n",
        "    print(f\"This label {item} was not found\")\n",
        "    return -1\n",
        "\n",
        "\n",
        "def get_model_bits(state_dict):\n",
        "    \"\"\"\n",
        "    :param state_dict: model object for which we want to get size in bits\n",
        "    :return: model_size - number of bites for all model's parameters\n",
        "    \"\"\"\n",
        "    torch.save(state_dict, \"temp.p\")\n",
        "    # Multiply by 8 to go from bytes to bits\n",
        "    model_size = os.path.getsize(\"temp.p\") * 8\n",
        "    os.remove('temp.p')\n",
        "    return model_size\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePo62fMS1TSp",
        "colab_type": "text"
      },
      "source": [
        "###QUANTIZATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdNFSP_G1SnU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "def quantize_float16(model_dict):\n",
        "    \"\"\"\n",
        "    :param model: Model's state dict with default 32-bit float parameters\n",
        "    :return: model's state dict with 16-bit float parameters\n",
        "    \"\"\"\n",
        "    for name, param in model_dict.items():\n",
        "        model_dict[name] = param.half()\n",
        "    return model_dict\n",
        "\n",
        "\n",
        "def quantize_int8(model_dict):\n",
        "    # Find maximum parameter\n",
        "    max_param = 0\n",
        "    for name, param in model_dict.items():\n",
        "        new_max = param.abs().max()\n",
        "        if new_max > max_param:\n",
        "            max_param = new_max\n",
        "    # Scale the maximum value to the max of an int8\n",
        "    multiplier = 127 / max_param\n",
        "    for name, param in model_dict.items():\n",
        "        model_dict[name] = (param * multiplier).to(torch.int8)\n",
        "    return model_dict, multiplier\n",
        "\n",
        "\n",
        "def decode_quantized_model_int8(model_dict, multiplier):\n",
        "    for name, param in model_dict.items():\n",
        "        model_dict[name] = param.to(torch.float32) / multiplier\n",
        "    return model_dict\n",
        "\n",
        "\n",
        "def no_quantization(model_dict):\n",
        "    return model_dict\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjOT9RKT1lCt",
        "colab_type": "text"
      },
      "source": [
        "###MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qilPkNt1kqX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch import nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "# https://arxiv.org/pdf/1602.05629.pdf\n",
        "# A CNN with two 5x5 convolution layers (the first with\n",
        "# 32 channels, the second with 64, each followed with 2x2\n",
        "# max pooling), a fully connected layer with 512 units and\n",
        "# ReLu activation, and a final softmax output layer (1,663,370\n",
        "# total parameters).\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 5)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 5)\n",
        "        self.fc1 = nn.Linear(4 * 4 * 64, 512)\n",
        "        self.fc2 = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = x.view(-1, 4 * 4 * 64)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tONngmM1eyD",
        "colab_type": "text"
      },
      "source": [
        "###TRAINING\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KCwK2211eIN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "class Client:\n",
        "    def __init__(self, data_loader, epochs=5):\n",
        "        self.data_loader = data_loader\n",
        "        self.epochs = epochs\n",
        "        self.lr = 0.001\n",
        "        self.log_interval = 5\n",
        "        self.seed = 42\n",
        "        torch.manual_seed(self.seed)\n",
        "        self.save_model = False\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model = CNN().to(self.device)\n",
        "        self.optimizer = optim.SGD(self.model.parameters(), lr=self.lr)\n",
        "        self.gradient_compression = None\n",
        "        self.criterion = torch.nn.CrossEntropyLoss()\n",
        "        self.model_name = \"mnist_cnn\"\n",
        "\n",
        "\n",
        "def train(client, epoch, logging=True):\n",
        "    # put model in train mode, we need gradients\n",
        "    client.model.train()\n",
        "    train_loader = client.data_loader\n",
        "    train_loss = 0\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.cuda(), target.cuda()\n",
        "        client.optimizer.zero_grad()\n",
        "        output = client.model(data)\n",
        "        # get the basic loss for our main task\n",
        "        total_loss = client.criterion(output, target)\n",
        "        total_loss.backward()\n",
        "        train_loss += total_loss.item()\n",
        "        client.optimizer.step()\n",
        "    _, train_accuracy = test(client, logging=False)\n",
        "    if logging:\n",
        "        print(f'Train Epoch: {epoch} Loss: {total_loss.item():.6f}, Train accuracy: {train_accuracy}')\n",
        "    return train_loss, train_accuracy\n",
        "\n",
        "\n",
        "def test(client, logging=True):\n",
        "    # put model in eval mode, disable dropout etc.\n",
        "    client.model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    test_loader = client.data_loader\n",
        "    # disable grad to perform testing quicker\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "            data, target = data.to(client.device), target.to(client.device)\n",
        "            output = client.model(data)\n",
        "            test_loss += client.criterion(output, target).item()\n",
        "            # prediction is an output with maximal probability\n",
        "            pred = output.argmax(1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    if logging:\n",
        "        print(f'Test set: Average loss: {test_loss:.4f}, '\n",
        "              f'Test accuracy: {correct} / {len(test_loader.dataset)} '\n",
        "              f'({test_accuracy:.0f}%)\\n')\n",
        "    return test_loss, test_accuracy\n",
        "\n",
        "\n",
        "def average_client_models(clients_dicts):\n",
        "    \"\"\"\n",
        "    :param clients_dicts: list of clients state dicts\n",
        "    :return: state_dict of averaged parameters\n",
        "    \"\"\"\n",
        "    # To perform averaging we need to go back to float32 cause summing is not supported for float16\n",
        "    for client in clients_dicts:\n",
        "        for name, param in client.items():\n",
        "            client[name] = param.float()\n",
        "    dict_keys = clients_dicts[0].keys()\n",
        "    final_dict = dict.fromkeys(dict_keys)\n",
        "    for key in dict_keys:\n",
        "        # Average model parameters\n",
        "        final_dict[key] = torch.cat([dictionary[key].unsqueeze(0) for dictionary in clients_dicts], dim=0).sum(0).div(\n",
        "            len(clients_dicts))\n",
        "    return final_dict\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-wtjQX-1sWP",
        "colab_type": "text"
      },
      "source": [
        "### EXPERIMENTS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUJUI7iy16aG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create outputs directory when running in colab for the first time\n",
        "if not os.path.isdir(\"./outputs\"):\n",
        "  os.mkdir(\"./outputs\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLhdO_zY1gy1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5f7314e7-3c6b-4147-bf26-e3729306f81f"
      },
      "source": [
        "import os\n",
        "import pickle\n",
        "from functools import reduce\n",
        "\n",
        "\n",
        "##############################\n",
        "# Configure here to get a specific experiment\n",
        "batch_size = 25\n",
        "num_clients = 5\n",
        "target_accuracy = 93\n",
        "iid_split = False\n",
        "non_iid_mix = 0\n",
        "# validation set\n",
        "percentage_val = 0\n",
        "# if use full MNIST with 60000 train and 10000 test\n",
        "full = True\n",
        "# default setup is 5 epochs per client,\n",
        "# here we have five clients therefore  we need [5, 5, 5, 5, 5]\n",
        "# change the list accordingly to get variable\n",
        "# number of epochs for different clients\n",
        "epochs_per_client = 5 * [5]\n",
        "quantization = no_quantization\n",
        "##############################\n",
        "\n",
        "\n",
        "# Load data\n",
        "train_loaders, _, test_loader = get_data_loaders(batch_size, num_clients, non_iid_mix=non_iid_mix,\n",
        "                                                 percentage_val=percentage_val, iid_split=iid_split, full=full)\n",
        "\n",
        "# Initialize all clients\n",
        "clients = [Client(train_loader, epochs) for train_loader, epochs in zip(train_loaders, epochs_per_client)]\n",
        "\n",
        "# Set seed for the script\n",
        "torch.manual_seed(clients[0].seed)\n",
        "\n",
        "testing_accuracy = 0\n",
        "num_rounds = 0\n",
        "\n",
        "central_server = Client(test_loader)\n",
        "\n",
        "experiment_state = {\"num_rounds\": 0,\n",
        "                    \"test_accuracies\": [],\n",
        "                    \"conserved_bits_from_server\": [],\n",
        "                    \"conserved_bits_from_clients\": [],\n",
        "                    \"transferred_bits_from_server\": [],\n",
        "                    \"transferred_bits_from_clients\": [],\n",
        "                    \"original_bits_from_server\": [],\n",
        "                    \"original_bits_from_clients\": []\n",
        "                    }\n",
        "\n",
        "while testing_accuracy < target_accuracy:\n",
        "    print(\"Communication Round {0}\".format(num_rounds+1))\n",
        "\n",
        "    if num_rounds > 1:\n",
        "        # Load server weights onto clients\n",
        "        for client in clients:\n",
        "            with torch.no_grad():\n",
        "                # Calculate number of bits in full server model\n",
        "                float_model_bits = get_model_bits(central_server.model.state_dict())\n",
        "                # Quantize server's model\n",
        "                quantized_model = quantization(central_server.model.state_dict())\n",
        "                bits_transferred = get_model_bits(quantized_model)\n",
        "                # Calculate how many bits we saved\n",
        "                bits_conserved = float_model_bits - bits_transferred\n",
        "                # Add to our summary\n",
        "                experiment_state[\"conserved_bits_from_server\"].append(bits_conserved)\n",
        "                experiment_state[\"transferred_bits_from_server\"].append(bits_transferred)\n",
        "                experiment_state[\"original_bits_from_server\"].append(float_model_bits)\n",
        "                # Distribute quantized model on clients\n",
        "                client.model.load_state_dict(quantized_model)\n",
        "\n",
        "    # Perform E local training steps for each client\n",
        "    for client_idx, client in enumerate(clients):\n",
        "        print(\"Training client {0}\".format(client_idx))\n",
        "        for epoch in range(1, client.epochs + 1):\n",
        "            train(client, epoch)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Get number of bits in all clients' models before quantization\n",
        "        clients_bits = reduce((lambda x, y: x * y), [get_model_bits(client.model.state_dict()) for client in clients])\n",
        "        # Quantize clients models\n",
        "        quantized_clients_models = [quantization(client.model.state_dict()) for client in clients]\n",
        "        quantized_clients_bits = reduce((lambda x, y: x * y),\n",
        "                                        [get_model_bits(client) for client in quantized_clients_models])\n",
        "        bits_conserved = (clients_bits - quantized_clients_bits) // num_clients\n",
        "        # Add to summary\n",
        "        experiment_state[\"conserved_bits_from_clients\"].append(bits_conserved // num_clients)\n",
        "        experiment_state[\"transferred_bits_from_clients\"].append(quantized_clients_bits // num_clients)\n",
        "        experiment_state[\"original_bits_from_clients\"].append(clients_bits // num_clients)\n",
        "        # Send quantized models to server and average them\n",
        "        averaged_model = average_client_models(quantized_clients_models)\n",
        "        central_server.model.load_state_dict(averaged_model)\n",
        "    # We have to convert back to float32 otherwise there is a mismatch with input dtype\n",
        "    central_server.model.to(torch.float32)\n",
        "    # Test the aggregated model\n",
        "    test_loss, testing_accuracy = test(central_server)\n",
        "    experiment_state['test_accuracies'].append(testing_accuracy)\n",
        "    experiment_state['num_rounds'] = num_rounds + 1\n",
        "    \n",
        "    num_rounds += 1\n",
        "\n",
        "# Save model\n",
        "if central_server.save_model:\n",
        "    torch.save(central_server.model.state_dict(), f\"{central_server.model_name}.pt\")\n",
        "\n",
        "# Save experiment states\n",
        "filename = f\"iid_split_{iid_split}_quantization_{quantization.__name__}.pkl\"\n",
        "with open(os.path.join(\"./outputs\", filename), \"wb\") as f:\n",
        "    pickle.dump(experiment_state, f)\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "* Using MNIST\n",
            "** Reducing the data-set, (use --full for the full thing)\n",
            "** Use 5000 train and 5000 test samples\n",
            "Communication Round 1\n",
            "Training client 0\n",
            "Train Epoch: 1 Loss: 0.607914, Train accuracy: 81.0\n",
            "Train Epoch: 2 Loss: 0.208089, Train accuracy: 95.4\n",
            "Train Epoch: 3 Loss: 0.185869, Train accuracy: 98.3\n",
            "Train Epoch: 4 Loss: 0.124611, Train accuracy: 99.3\n",
            "Train Epoch: 5 Loss: 0.039825, Train accuracy: 99.6\n",
            "Training client 1\n",
            "Train Epoch: 1 Loss: 0.928907, Train accuracy: 77.1\n",
            "Train Epoch: 2 Loss: 0.194541, Train accuracy: 93.2\n",
            "Train Epoch: 3 Loss: 0.116740, Train accuracy: 97.1\n",
            "Train Epoch: 4 Loss: 0.111299, Train accuracy: 98.4\n",
            "Train Epoch: 5 Loss: 0.038128, Train accuracy: 99.2\n",
            "Training client 2\n",
            "Train Epoch: 1 Loss: 0.806540, Train accuracy: 84.4\n",
            "Train Epoch: 2 Loss: 0.267115, Train accuracy: 90.1\n",
            "Train Epoch: 3 Loss: 0.347859, Train accuracy: 94.4\n",
            "Train Epoch: 4 Loss: 0.359761, Train accuracy: 97.6\n",
            "Train Epoch: 5 Loss: 0.173991, Train accuracy: 98.5\n",
            "Training client 3\n",
            "Train Epoch: 1 Loss: 1.574458, Train accuracy: 66.7\n",
            "Train Epoch: 2 Loss: 0.479374, Train accuracy: 89.3\n",
            "Train Epoch: 3 Loss: 0.208241, Train accuracy: 93.5\n",
            "Train Epoch: 4 Loss: 0.219497, Train accuracy: 96.8\n",
            "Train Epoch: 5 Loss: 0.168015, Train accuracy: 98.8\n",
            "Training client 4\n",
            "Train Epoch: 1 Loss: 0.588063, Train accuracy: 87.1\n",
            "Train Epoch: 2 Loss: 0.244336, Train accuracy: 95.5\n",
            "Train Epoch: 3 Loss: 0.143994, Train accuracy: 98.0\n",
            "Train Epoch: 4 Loss: 0.119971, Train accuracy: 99.4\n",
            "Train Epoch: 5 Loss: 0.058359, Train accuracy: 99.8\n",
            "Test set: Average loss: 0.0920, Test accuracy: 641 / 5000 (13%)\n",
            "\n",
            "Communication Round 2\n",
            "Training client 0\n",
            "Train Epoch: 1 Loss: 0.021789, Train accuracy: 99.9\n",
            "Train Epoch: 2 Loss: 0.028744, Train accuracy: 100.0\n",
            "Train Epoch: 3 Loss: 0.026506, Train accuracy: 100.0\n",
            "Train Epoch: 4 Loss: 0.022301, Train accuracy: 100.0\n",
            "Train Epoch: 5 Loss: 0.013930, Train accuracy: 100.0\n",
            "Training client 1\n",
            "Train Epoch: 1 Loss: 0.008618, Train accuracy: 99.6\n",
            "Train Epoch: 2 Loss: 0.044062, Train accuracy: 99.8\n",
            "Train Epoch: 3 Loss: 0.023434, Train accuracy: 99.9\n",
            "Train Epoch: 4 Loss: 0.041618, Train accuracy: 100.0\n",
            "Train Epoch: 5 Loss: 0.028159, Train accuracy: 100.0\n",
            "Training client 2\n",
            "Train Epoch: 1 Loss: 0.056364, Train accuracy: 99.7\n",
            "Train Epoch: 2 Loss: 0.051552, Train accuracy: 99.8\n",
            "Train Epoch: 3 Loss: 0.061274, Train accuracy: 99.9\n",
            "Train Epoch: 4 Loss: 0.030523, Train accuracy: 100.0\n",
            "Train Epoch: 5 Loss: 0.019017, Train accuracy: 100.0\n",
            "Training client 3\n",
            "Train Epoch: 1 Loss: 0.102414, Train accuracy: 98.9\n",
            "Train Epoch: 2 Loss: 0.125629, Train accuracy: 99.7\n",
            "Train Epoch: 3 Loss: 0.026385, Train accuracy: 100.0\n",
            "Train Epoch: 4 Loss: 0.013211, Train accuracy: 99.9\n",
            "Train Epoch: 5 Loss: 0.045541, Train accuracy: 100.0\n",
            "Training client 4\n",
            "Train Epoch: 1 Loss: 0.121992, Train accuracy: 99.7\n",
            "Train Epoch: 2 Loss: 0.023915, Train accuracy: 100.0\n",
            "Train Epoch: 3 Loss: 0.021852, Train accuracy: 100.0\n",
            "Train Epoch: 4 Loss: 0.006571, Train accuracy: 100.0\n",
            "Train Epoch: 5 Loss: 0.025145, Train accuracy: 100.0\n",
            "Test set: Average loss: 0.0920, Test accuracy: 701 / 5000 (14%)\n",
            "\n",
            "Communication Round 3\n",
            "Training client 0\n",
            "Train Epoch: 1 Loss: 2.263607, Train accuracy: 33.7\n",
            "Train Epoch: 2 Loss: 2.149342, Train accuracy: 55.0\n",
            "Train Epoch: 3 Loss: 1.092574, Train accuracy: 75.7\n",
            "Train Epoch: 4 Loss: 0.537845, Train accuracy: 85.4\n",
            "Train Epoch: 5 Loss: 0.485708, Train accuracy: 87.1\n",
            "Training client 1\n",
            "Train Epoch: 1 Loss: 2.235360, Train accuracy: 35.6\n",
            "Train Epoch: 2 Loss: 2.024980, Train accuracy: 48.0\n",
            "Train Epoch: 3 Loss: 1.253228, Train accuracy: 76.2\n",
            "Train Epoch: 4 Loss: 0.613576, Train accuracy: 79.3\n",
            "Train Epoch: 5 Loss: 0.566080, Train accuracy: 88.2\n",
            "Training client 2\n",
            "Train Epoch: 1 Loss: 2.255331, Train accuracy: 36.6\n",
            "Train Epoch: 2 Loss: 2.019697, Train accuracy: 42.0\n",
            "Train Epoch: 3 Loss: 1.279631, Train accuracy: 80.5\n",
            "Train Epoch: 4 Loss: 0.782577, Train accuracy: 81.3\n",
            "Train Epoch: 5 Loss: 0.247692, Train accuracy: 89.7\n",
            "Training client 3\n",
            "Train Epoch: 1 Loss: 2.268642, Train accuracy: 35.7\n",
            "Train Epoch: 2 Loss: 2.013822, Train accuracy: 49.3\n",
            "Train Epoch: 3 Loss: 1.123499, Train accuracy: 74.6\n",
            "Train Epoch: 4 Loss: 0.583122, Train accuracy: 82.0\n",
            "Train Epoch: 5 Loss: 0.548257, Train accuracy: 84.6\n",
            "Training client 4\n",
            "Train Epoch: 1 Loss: 2.277911, Train accuracy: 31.2\n",
            "Train Epoch: 2 Loss: 2.095429, Train accuracy: 58.1\n",
            "Train Epoch: 3 Loss: 1.289485, Train accuracy: 73.8\n",
            "Train Epoch: 4 Loss: 0.606448, Train accuracy: 83.6\n",
            "Train Epoch: 5 Loss: 0.370743, Train accuracy: 87.1\n",
            "Test set: Average loss: 0.0242, Test accuracy: 4159 / 5000 (83%)\n",
            "\n",
            "Communication Round 4\n",
            "Training client 0\n",
            "Train Epoch: 1 Loss: 0.855828, Train accuracy: 90.3\n",
            "Train Epoch: 2 Loss: 0.106402, Train accuracy: 92.4\n",
            "Train Epoch: 3 Loss: 0.320883, Train accuracy: 93.3\n",
            "Train Epoch: 4 Loss: 0.317913, Train accuracy: 94.0\n",
            "Train Epoch: 5 Loss: 0.085244, Train accuracy: 95.0\n",
            "Training client 1\n",
            "Train Epoch: 1 Loss: 0.408370, Train accuracy: 89.4\n",
            "Train Epoch: 2 Loss: 0.359022, Train accuracy: 90.7\n",
            "Train Epoch: 3 Loss: 0.247520, Train accuracy: 93.1\n",
            "Train Epoch: 4 Loss: 0.094659, Train accuracy: 96.4\n",
            "Train Epoch: 5 Loss: 0.378907, Train accuracy: 96.7\n",
            "Training client 2\n",
            "Train Epoch: 1 Loss: 0.622289, Train accuracy: 88.4\n",
            "Train Epoch: 2 Loss: 0.328492, Train accuracy: 92.6\n",
            "Train Epoch: 3 Loss: 0.144422, Train accuracy: 93.9\n",
            "Train Epoch: 4 Loss: 0.279456, Train accuracy: 92.9\n",
            "Train Epoch: 5 Loss: 0.222998, Train accuracy: 94.8\n",
            "Training client 3\n",
            "Train Epoch: 1 Loss: 0.360275, Train accuracy: 91.9\n",
            "Train Epoch: 2 Loss: 0.671653, Train accuracy: 90.7\n",
            "Train Epoch: 3 Loss: 0.254186, Train accuracy: 94.3\n",
            "Train Epoch: 4 Loss: 0.183930, Train accuracy: 94.4\n",
            "Train Epoch: 5 Loss: 0.143870, Train accuracy: 96.2\n",
            "Training client 4\n",
            "Train Epoch: 1 Loss: 0.411563, Train accuracy: 89.6\n",
            "Train Epoch: 2 Loss: 0.384991, Train accuracy: 89.5\n",
            "Train Epoch: 3 Loss: 0.182805, Train accuracy: 92.4\n",
            "Train Epoch: 4 Loss: 0.158955, Train accuracy: 93.7\n",
            "Train Epoch: 5 Loss: 0.214115, Train accuracy: 94.8\n",
            "Test set: Average loss: 0.0129, Test accuracy: 4519 / 5000 (90%)\n",
            "\n",
            "Communication Round 5\n",
            "Training client 0\n",
            "Train Epoch: 1 Loss: 0.333745, Train accuracy: 93.2\n",
            "Train Epoch: 2 Loss: 0.722083, Train accuracy: 94.5\n",
            "Train Epoch: 3 Loss: 0.154913, Train accuracy: 95.8\n",
            "Train Epoch: 4 Loss: 0.200478, Train accuracy: 96.2\n",
            "Train Epoch: 5 Loss: 0.528241, Train accuracy: 95.5\n",
            "Training client 1\n",
            "Train Epoch: 1 Loss: 0.300669, Train accuracy: 93.9\n",
            "Train Epoch: 2 Loss: 0.383288, Train accuracy: 96.4\n",
            "Train Epoch: 3 Loss: 0.063915, Train accuracy: 97.2\n",
            "Train Epoch: 4 Loss: 0.186126, Train accuracy: 97.4\n",
            "Train Epoch: 5 Loss: 0.168618, Train accuracy: 97.8\n",
            "Training client 2\n",
            "Train Epoch: 1 Loss: 0.254708, Train accuracy: 94.2\n",
            "Train Epoch: 2 Loss: 0.124670, Train accuracy: 95.3\n",
            "Train Epoch: 3 Loss: 0.087946, Train accuracy: 95.6\n",
            "Train Epoch: 4 Loss: 0.145423, Train accuracy: 96.0\n",
            "Train Epoch: 5 Loss: 0.110520, Train accuracy: 95.8\n",
            "Training client 3\n",
            "Train Epoch: 1 Loss: 0.501658, Train accuracy: 95.2\n",
            "Train Epoch: 2 Loss: 0.458491, Train accuracy: 94.9\n",
            "Train Epoch: 3 Loss: 0.170627, Train accuracy: 96.2\n",
            "Train Epoch: 4 Loss: 0.227285, Train accuracy: 96.3\n",
            "Train Epoch: 5 Loss: 0.031612, Train accuracy: 97.4\n",
            "Training client 4\n",
            "Train Epoch: 1 Loss: 0.678088, Train accuracy: 89.9\n",
            "Train Epoch: 2 Loss: 0.116811, Train accuracy: 94.6\n",
            "Train Epoch: 3 Loss: 0.175257, Train accuracy: 94.0\n",
            "Train Epoch: 4 Loss: 0.091591, Train accuracy: 96.0\n",
            "Train Epoch: 5 Loss: 0.059021, Train accuracy: 96.7\n",
            "Test set: Average loss: 0.0100, Test accuracy: 4618 / 5000 (92%)\n",
            "\n",
            "Communication Round 6\n",
            "Training client 0\n",
            "Train Epoch: 1 Loss: 0.216093, Train accuracy: 93.2\n",
            "Train Epoch: 2 Loss: 0.057641, Train accuracy: 96.4\n",
            "Train Epoch: 3 Loss: 0.105486, Train accuracy: 97.3\n",
            "Train Epoch: 4 Loss: 0.085888, Train accuracy: 97.8\n",
            "Train Epoch: 5 Loss: 0.038583, Train accuracy: 97.0\n",
            "Training client 1\n",
            "Train Epoch: 1 Loss: 0.213881, Train accuracy: 93.2\n",
            "Train Epoch: 2 Loss: 0.175378, Train accuracy: 97.7\n",
            "Train Epoch: 3 Loss: 0.055748, Train accuracy: 97.9\n",
            "Train Epoch: 4 Loss: 0.053312, Train accuracy: 97.7\n",
            "Train Epoch: 5 Loss: 0.116782, Train accuracy: 98.4\n",
            "Training client 2\n",
            "Train Epoch: 1 Loss: 0.075619, Train accuracy: 95.3\n",
            "Train Epoch: 2 Loss: 0.119274, Train accuracy: 96.6\n",
            "Train Epoch: 3 Loss: 0.235847, Train accuracy: 97.3\n",
            "Train Epoch: 4 Loss: 0.188730, Train accuracy: 97.7\n",
            "Train Epoch: 5 Loss: 0.234656, Train accuracy: 97.2\n",
            "Training client 3\n",
            "Train Epoch: 1 Loss: 0.025558, Train accuracy: 96.6\n",
            "Train Epoch: 2 Loss: 0.146674, Train accuracy: 96.7\n",
            "Train Epoch: 3 Loss: 0.351312, Train accuracy: 97.1\n",
            "Train Epoch: 4 Loss: 0.123096, Train accuracy: 97.6\n",
            "Train Epoch: 5 Loss: 0.052690, Train accuracy: 98.2\n",
            "Training client 4\n",
            "Train Epoch: 1 Loss: 0.574557, Train accuracy: 94.7\n",
            "Train Epoch: 2 Loss: 0.093395, Train accuracy: 95.9\n",
            "Train Epoch: 3 Loss: 0.116050, Train accuracy: 96.2\n",
            "Train Epoch: 4 Loss: 0.327032, Train accuracy: 97.0\n",
            "Train Epoch: 5 Loss: 0.055268, Train accuracy: 97.5\n",
            "Test set: Average loss: 0.0085, Test accuracy: 4667 / 5000 (93%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}